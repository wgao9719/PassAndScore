# Phase 2: Supervisor Training with Frozen Players
#
# Prerequisites:
#   - Must have trained players with Phase 1 (diverse behaviors via FiLM + discriminator)
#   - Load Phase 1 checkpoint before starting Phase 2
#
# This configuration:
#   - Freezes player networks (actor, critic, backbone, discriminator)
#   - Trains ONLY the supervisor: c_t = F_φ(s_t^global)
#   - Supervisor learns which strategy to select based on game state
#   - Trained via PPO on team reward
#
# The supervisor is now "turning a wheel connected to behavior" because:
#   - Phase 1 ensured (o, c) → a is sensitive to c
#   - Different c give different outcomes
#   - Gradients for φ are non-degenerate

expr_group: gr_football
expr_name: full_game_5_vs_5_hard_mappo_phase2
log_dir: ./logs
seed: 0
eval_only: False

distributed:
  use: False
  auto_connect:
  auto_copy:
  nodes:
    master:
      ip: "auto"
    workers:      
      - ip:

framework:
  name: "psro"
  max_rounds: 1
  meta_solver: "nash"
  sync_training: True
  stopper:
    type: "win_rate_stopper"
    kwargs:
      min_win_rate: 1.0
      max_steps: 400

agent_manager:
  num_agents: 2
  share_policies: False

evaluation_manager:
  num_eval_rollouts: 1

policy_data_manager:
  update_func: "gr_football" 
  fields:
    payoff:
      type: "matrix"
      missing_value: -100 
    score:
      type: "matrix"
      missing_value: -100 
    win:
      type: "matrix"
      missing_value: -100 
    lose:
      type: "matrix"
      missing_value: -100 
    my_goal:
      type: "matrix"
      missing_value: -100 
    goal_diff:
      type: "matrix"
      missing_value: -100 
        
monitor:
  type: "local"
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]

rollout_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  num_workers: 4
  seed: 12345
  saving_interval: 20
  batch_size: ${rollout_manager.num_workers}
  eval_batch_size: 16
  eval_freq: 100
  rollout_metric_cfgs:
    reward:
      type: "sliding"
      window_size: 20
      init_list: [-10000]
    win:
      type: "sliding"
      window_size: 20
      init_list: [0,0,0,0,0]
  worker:
    distributed:
      resources:
        num_cpus: 1
    rollout_length: 3001
    eval_rollout_length: 3001
    sample_length: 0
    padding_length:
    # NOTE: flash_token DISABLED for Phase 2
    # Supervisor controls behavior via strategy_code only
    flash_token:
      enabled: False
      dim: 0
      mode: "off"
      std: 1.0
    # ===== Phase 2: Supervisor selects strategy =====
    strategy_conditioning:
      enabled: True
      training_phase: "phase2"      # Supervisor-driven strategy selection
      num_strategies: 8
      intrinsic_reward_coef: 0.0    # No intrinsic reward in Phase 2 (pure env reward)
      window_size: 16
      # Temporal abstraction: supervisor acts every K steps
      # Prevents "jitter" - gives strategies time to manifest
      # 50 steps = ~3-5 seconds in GRF at 10Hz
      supervisor_interval: 50
      # Dense tactical rewards for supervisor (prevents posterior collapse)
      tactical_rewards:
        enabled: True
        goal_coef: 1.0              # Weight for goal/score rewards
        progression_coef: 0.1       # Weight for ball moving toward enemy goal
        possession_coef: 0.05       # Weight for keeping the ball
    rollout_func_name: "rollout_func"
    episode_mode: 'traj'
    envs:
      - cls: "gr_football"
        id_prefix: "gr_footbal"
        scenario_config:
          env_name: "benchmark_full_game_5_vs_5_hard"
          number_of_left_players_agent_controls: 4
          number_of_right_players_agent_controls: 1
          representation: "raw"
          rewards: "scoring,checkpoints"
          stacked: False
          logdir: '/tmp/football/malib_phase2'
          write_goal_dumps: False
          write_full_episode_dumps: False
          render: False
          other_config_options:
            action_set: v2
        reward_config:
          goal_reward: 0
          official_reward: 1
    credit_reassign:
    decaying_exploration:
      init_noise: 0.1
      total_epoch_to_zero: 2000
      interval: 400

training_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  master_addr: "127.0.0.1"
  master_port: "12776"
  local_queue_size: 1
  batch_size: ${rollout_manager.batch_size}
  num_prefetchers: 1
  data_prefetcher:
    distributed:
      resources:
        num_cpus: 1
  num_trainers: 1
  update_interval: 1
  gpu_preload: False
  trainer:
    distributed:
      resources:
        num_cpus: 1
        num_gpus: 0
        resources:
          - ["node:${distributed.nodes.master.ip}",0.01]
    optimizer: "Adam"
    actor_lr: 5.e-4               # Not used (players frozen)
    critic_lr: 5.e-4              # Not used (players frozen)
    backbone_lr: 5.e-4            # Not used (players frozen)
    supervisor_lr: 5.e-4          # Supervisor learning rate
    opti_eps: 1.e-5
    weight_decay: 0.0
    lr_decay: False
    lr_decay_epoch: 2000
    device: "cpu"

data_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  table_cfg:
    capacity: 1000
    sampler_type: "lumrf"
    sample_max_usage: 1
    rate_limiter_cfg:
      min_size: ${training_manager.batch_size}
  read_timeout: 1

policy_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]

populations:
  - population_id: default
    algorithm:
      name: "MAPPO"
      model_config:
        model: "gr_football.passandscore_factorized"
        initialization:
          use_orthogonal: True
          gain: 1.
        backbone: {}
        actor:
          network: mlp
          layers:
            - units: 128
              activation: ReLU
            - units: 128
              activation: ReLU
          output:
            activation: False
        critic:
          network: mlp
          layers:
            - units: 128
              activation: ReLU
            - units: 128
              activation: ReLU
          output:
            activation: False

      custom_config:
        FE_cfg:
          num_players: 10
        sub_algorithm_name: MAPPO
        num_agents: 4
        
        # CTDE Configuration
        factorized_actor_use_global: False
        factorized_critic_use_local: False
        factorized_critic_use_pooled: True
        # flash_token disabled for Phase 2 (supervisor controls via strategy_code)
        factorized_actor_use_flash: False
        factorized_critic_use_flash: False
        flash_dim: 0
        
        # ===== Phase 2: Supervisor Training =====
        training_phase: "phase2"            # Phase 2 mode
        use_strategy_conditioning: True     # FiLM conditioning active
        use_supervisor: True                # Enable supervisor network
        num_strategies: 8
        strategy_embed_dim: 32
        discriminator_hidden_dim: 64
        discriminator_window_size: 16
        discriminator_use_actions: True
        
        # Supervisor "God-View" Architecture (Transformer-based)
        # Preserves spatial/relational info - no pooling!
        supervisor_hidden_dim: 128          # Transformer hidden dimension
        supervisor_use_transformer: True    # Use Transformer (vs MLP fallback)
        supervisor_entity_dim: 6            # Features per entity (x,y,vx,vy,dir,tired)
        supervisor_num_entities: 23         # Ball + 22 players
        supervisor_num_heads: 4             # Attention heads
        supervisor_num_layers: 2            # Transformer layers
        
        other_clip_param: 0.125

        gamma: 0.995
        use_cuda: False
        device: "cpu"
        use_q_head: False
        ppo_epoch: 10
        num_mini_batch: 2
        
        return_mode: new_gae
        gae:
          gae_lambda: 0.95
        vtrace:
          clip_rho_threshold: 1.0
          clip_pg_rho_threshold: 100.0

        use_rnn: False
        rnn_layer_num: 1
        rnn_data_chunk_length: 16

        use_feature_normalization: True
        use_popart: True
        popart_beta: 0.99999

        entropy_coef: 0.005
        clip_param: 0.2

        use_modified_mappo: False

      policy_init_cfg:
        agent_0:
          new_policy_ctr_start: -1
          init_cfg:
            # IMPORTANT: Load Phase 1 trained checkpoint here
            - condition: "==0"
              strategy: pretrained
              policy_id: phase1_trained
              policy_dir: # TODO: Set path to Phase 1 checkpoint
          initial_policies:

        agent_1:
          new_policy_ctr_start: -1
          init_cfg:
            - condition: "==0"
              strategy: pretrained
              policy_id: built_in
              policy_dir: light_malib/trained_models/gr_football/5_vs_5/built_in
          initial_policies:

